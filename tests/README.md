# Test Suite

This directory contains the comprehensive test suite for the SimpleSim simulation system.

## Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures and test configuration
â”œâ”€â”€ unit/                    # Unit tests for individual components
â”‚   â””â”€â”€ test_config_service.py
â”œâ”€â”€ integration/             # Integration tests for API endpoints
â”‚   â””â”€â”€ test_api_endpoints.py
â”œâ”€â”€ verification/            # End-to-end verification tests
â”‚   â”œâ”€â”€ test_comprehensive_kpi.py
â”‚   â””â”€â”€ test_recruitment_mechanics.py
â”œâ”€â”€ fixtures/                # Test data and fixtures
â””â”€â”€ archive/                 # Legacy tests (for reference)
```

## Test Categories

### Unit Tests (`tests/unit/`)
- **Purpose**: Test individual components in isolation
- **Coverage**: ConfigService, SimulationEngine, KPIService
- **Dependencies**: Minimal, no external services required

### Integration Tests (`tests/integration/`)
- **Purpose**: Test API endpoints and service interactions
- **Coverage**: All API routes, request/response handling
- **Dependencies**: Backend server must be running

### Verification Tests (`tests/verification/`)
- **Purpose**: End-to-end verification of business logic
- **Coverage**: Complete simulation flows, KPI calculations, recruitment mechanics
- **Dependencies**: Full system (backend + configuration)

## Running Tests

### Prerequisites
1. Backend server must be running on port 8000
2. Configuration file must be loaded with test data
3. Python dependencies installed

### Quick Start
```bash
# Run all tests (excluding archive)
python3 -m pytest tests/ -v

# Run specific test categories
python3 -m pytest tests/unit/ -v                    # Unit tests only
python3 -m pytest tests/integration/ -v             # Integration tests only
python3 -m pytest tests/verification/ -v            # Verification tests only

# Run with coverage
python3 -m pytest tests/ --cov=backend/src --cov-report=html

# Run specific test file
python3 -m pytest tests/unit/test_config_service.py -v

# Run specific test method
python3 -m pytest tests/verification/test_comprehensive_kpi.py::TestComprehensiveKPI::test_full_kpi_verification_flow -v
```

### Test Configuration
- **pytest.ini**: Main pytest configuration
- **conftest.py**: Shared fixtures and test helpers
- **Timeout**: Integration tests have 120-180 second timeouts for simulations

## Test Fixtures

### Core Fixtures (`conftest.py`)
- `api_client`: HTTP client for API testing
- `ensure_server_running`: Ensures backend is available
- `test_config`: Standard simulation configuration
- `stockholm_consultant_a_config`: Stockholm A-level configuration helper
- `test_helper`: Utility functions for test data extraction
- `logger`: Structured logging for test output

### Test Data
- Configuration files in `config/`
- Excel test files in `scripts/`
- Mock data generated by fixtures

## Test Coverage

### Current Coverage
- âœ… **ConfigService**: Full unit test coverage
- âœ… **API Endpoints**: All major endpoints tested
- âœ… **Simulation Engine**: End-to-end verification
- âœ… **KPI Calculations**: Comprehensive KPI verification
- âœ… **Recruitment Mechanics**: Dynamic testing with realistic tolerances

### Test Results
- **Unit Tests**: 9/9 passing
- **Integration Tests**: 8/8 passing (1 skipped)
- **Verification Tests**: 3/3 passing
- **Total**: 20/20 passing tests

## Troubleshooting

### Common Issues

**Backend Not Running**
```bash
# Start backend server
PYTHONPATH=. python3 -m uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

**Configuration Issues**
```bash
# Verify configuration is loaded
curl http://localhost:8000/offices/config
```

**Test Timeouts**
- Increase timeout in test configuration
- Check server performance
- Verify simulation parameters

**Import Errors**
```bash
# Ensure PYTHONPATH is set
export PYTHONPATH=.
python3 -m pytest tests/ -v
```

### Debug Mode
```bash
# Run with detailed output
python3 -m pytest tests/ -v -s --tb=long

# Run single test with debug
python3 -m pytest tests/verification/test_comprehensive_kpi.py::TestComprehensiveKPI::test_full_kpi_verification_flow -v -s
```

## Adding New Tests

### Unit Test Template
```python
def test_component_functionality(self, component_fixture):
    """Test description"""
    # Arrange
    input_data = "test_data"
    
    # Act
    result = component_fixture.method(input_data)
    
    # Assert
    assert result is not None
    assert result == expected_value
```

### Integration Test Template
```python
def test_api_endpoint(self, api_client, ensure_server_running):
    """Test API endpoint"""
    response = api_client.get("/endpoint")
    
    assert response.status_code == 200
    data = response.json()
    assert "expected_field" in data
```

### Verification Test Template
```python
def test_business_logic(self, api_client, ensure_server_running, test_config, logger):
    """Test business logic end-to-end"""
    logger("ðŸ§ª TEST DESCRIPTION")
    
    # Setup
    # Execute
    # Verify
    # Cleanup
```

## Best Practices

1. **Use Fixtures**: Leverage shared fixtures for common setup
2. **Realistic Tolerances**: Use appropriate tolerances for real-world data
3. **Clean State**: Ensure tests don't interfere with each other
4. **Meaningful Assertions**: Test business logic, not implementation details
5. **Structured Logging**: Use logger fixture for clear test output
6. **Timeout Handling**: Set appropriate timeouts for long-running operations

## Legacy Tests

The `tests/archive/` directory contains legacy tests that were refactored during the test reorganization. These tests are kept for reference but are not part of the active test suite.

## Continuous Integration

Tests are designed to run in CI/CD pipelines:
- No external dependencies beyond the application
- Deterministic results
- Clear pass/fail criteria
- Comprehensive coverage of critical paths
